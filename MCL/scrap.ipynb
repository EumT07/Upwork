{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd42b21f",
   "metadata": {},
   "source": [
    "### Install requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5c54e38",
   "metadata": {},
   "source": [
    "### ✅ Virtual env\n",
    "\n",
    "Virtual environment allows to manage dependencies separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4f4634",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "py -m venv scrap\n",
    "\n",
    "#or\n",
    "\n",
    "python -m venv scrap"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcd512",
   "metadata": {},
   "source": [
    "### ✅ Beautiful Soup : Scrap Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76baa799",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install BeautifulSoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cdc76fe",
   "metadata": {},
   "source": [
    "### ✅ Pandas - Matplotlib - seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "797a75e5",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install pandas\n",
    "pip install mtplotlib\n",
    "pip install seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f358fc60",
   "metadata": {},
   "source": [
    "### ✅ Install openpyxl : To be able to apply and use excel formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce18feed",
   "metadata": {
    "vscode": {
     "languageId": "shellscript"
    }
   },
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ed99fc",
   "metadata": {},
   "source": [
    "### Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61cc3094",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time, os\n",
    "import json\n",
    "import pandas as pd\n",
    "from typing import List, Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a891e26",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e143ff8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "products_list: List[Dict] = []\n",
    "headers = {\n",
    "        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',\n",
    "        'Accept-Language': 'es-MX,es;q=0.9'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c99b523",
   "metadata": {},
   "source": [
    "### Items to search: Laptops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93b6e4a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping completed.\n"
     ]
    }
   ],
   "source": [
    "class Scrap():\n",
    "\n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the scraper and start the scraping process\"\"\"\n",
    "        self.get_html()\n",
    "\n",
    "    def get_html(self):\n",
    "        \"\"\"\n",
    "        Retrieve HTML from target pages\n",
    "        Handles errors and completes the process\n",
    "        \"\"\"\n",
    "        try:\n",
    "            \n",
    "            urls = self.range_pages()\n",
    "\n",
    "            for url in urls:\n",
    "                response = requests.get(url, headers=headers)\n",
    "                html = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "                self.get_items(html)\n",
    "                time.sleep(1)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"get_hmtl | Error: {e}\")\n",
    "            return None\n",
    "        finally:\n",
    "            print(\"Scraping completed.\")\n",
    "            time.sleep(2)\n",
    "    \n",
    "    def range_pages(self):\n",
    "        \"\"\"\n",
    "        Generate URLs for all pages to be scraped\n",
    "        \n",
    "        Returns:\n",
    "            List[str]: List of generated URLs\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # I did this count manually because there were not many pages, but I could have done it automatically.\n",
    "            base_url: str = \"https://listado.mercadolibre.com.ve/laptops\"\n",
    "            url_list: List[str] = [base_url]\n",
    "            total_pages: int = 41\n",
    "            total_items: int = 49\n",
    "\n",
    "            for page in range(1, total_pages):\n",
    "                pagination_url = f\"{base_url}-accesorios/laptops_Desde_{total_items}_NoIndex_True\"\n",
    "                url_list.append(pagination_url)\n",
    "                total_items += 48\n",
    "            \n",
    "            return url_list\n",
    "        except Exception as e:\n",
    "            print(f\"range_page | Error: {e}\")\n",
    "            return None\n",
    "        \n",
    "    def get_text_item(self, item: str, html_tag: str, html_class: str, default: str = \"N/A\"):\n",
    "\n",
    "        \"\"\"\n",
    "        Extract text content from HTML element\n",
    "        \n",
    "        Args:\n",
    "            item: BeautifulSoup element to search within\n",
    "            html_tag: HTML tag name to search for\n",
    "            html_class: CSS class name of the element\n",
    "            default: Default value if element not found\n",
    "            \n",
    "        Returns:\n",
    "            str: Extracted text or default value\n",
    "        \"\"\"\n",
    "        try:\n",
    "            element = item.find(html_tag, class_=html_class) if item else None\n",
    "            return element.get_text(strip=True) if element else default\n",
    "        except Exception as e:\n",
    "            print(f\" get_text_item | Error: {e}\")\n",
    "            return \"None\"\n",
    "\n",
    "    def get_attribute(self, item: str, html_tag: str, attribute: str, default: str = \"N/A\"):\n",
    "        \"\"\"\n",
    "        Extract attribute value from HTML element\n",
    "        \n",
    "        Args:\n",
    "            item: BeautifulSoup element to search within\n",
    "            html_tag: HTML tag name to search for\n",
    "            attribute: Attribute name to extract (src/href)\n",
    "            default: Default value if attribute not found\n",
    "            \n",
    "        Returns:\n",
    "            str: Attribute value or default\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # get img urls\n",
    "            if attribute == 'src':\n",
    "                element = item.select_one(html_tag) if item else None\n",
    "                return  element.get('data-src') or element.get(attribute)\n",
    "            \n",
    "            # get product urls\n",
    "            if attribute == 'href':\n",
    "                element = item.select_one(html_tag) if item else None\n",
    "                return element.get(attribute, default)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\" get_attribute | Error: {e}\")\n",
    "            return f\"None | atrribute: {attribute}\"\n",
    "\n",
    "    def get_items(self, html):\n",
    "        \"\"\"\n",
    "        Extract all product items from HTML\n",
    "\n",
    "        Args:\n",
    "            html: BeautifulSoup parsed HTML content\n",
    "        \"\"\"\n",
    "        if html is None:\n",
    "            print(\"Failed to retrieve HTML.\")\n",
    "            return\n",
    "        \n",
    "        items = html.find_all('li', class_='ui-search-layout__item')\n",
    "\n",
    "        for item in items:\n",
    "            try:\n",
    "                products_list.append({\n",
    "                        \"brand\": self.get_text_item(item,'span',\"poly-component__brand\"),\n",
    "                        \"description\": self.get_text_item(item, 'a','poly-component__title'),\n",
    "                        \"img_url\": self.get_attribute(item,'img', 'src'),\n",
    "                        \"product_url\": self.get_attribute(item,'a','href'),\n",
    "                        \"seller\": self.get_text_item(item,'span','poly-component__seller'),\n",
    "                        \"price\": {\n",
    "                            \"amount\": self.get_text_item(item,'span','andes-money-amount__fraction'),\n",
    "                            \"discount\": self.get_text_item(item,'span','andes-money-amount__discount'),    \n",
    "                        },\n",
    "                        \"reviews\": {\n",
    "                            \"rating\": self.get_text_item(item,'span','poly-reviews__rating'),\n",
    "                            \"stars\": True if len(item.find_all('span', class_='poly-reviews__starts')) > 0 else False,\n",
    "                            'total_sales': self.get_text_item(item,'span','poly-reviews__total').strip(\"()\")\n",
    "                        },\n",
    "                        \"shipping\": self.get_text_item(item,'div','poly-component__shipping')\n",
    "                })\n",
    "                time.sleep(1)\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {e}\")\n",
    "\n",
    "run = Scrap()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1919e88c",
   "metadata": {},
   "source": [
    "### Converting Data --> JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad64b06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_JSON(file,laptops):\n",
    "    \"\"\"\n",
    "    Convert a list of product dictionaries into a JSON format.\n",
    "    \n",
    "    Args:\n",
    "        products: List of product dictionaries containing scraped data\n",
    "        \n",
    "    Returns:\n",
    "        JSON: JSON formatted string of the product data\n",
    "    \"\"\"\n",
    "    # root\n",
    "    root = None\n",
    "\n",
    "    # Make folder\n",
    "    folder_path = \"./products\"\n",
    "    os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "    #file root\n",
    "    root = os.path.join(f\"{folder_path}/{file}.json\")\n",
    "    \n",
    "    with open(root, \"w\") as f:\n",
    "        json.dump([],f)\n",
    "\n",
    "    def insert_JSON(data, fileName = root):\n",
    "        with open(fileName, \"r+\", encoding= \"utf-8\") as file:\n",
    "            file_data = json.load(file)\n",
    "            file_data.append(data)\n",
    "            file.seek(0)\n",
    "            json.dump(file_data, file, indent= 4)\n",
    "\n",
    "    for laptop in laptops:\n",
    "        insert_JSON(laptop)\n",
    "\n",
    "Get_JSON(\"laptops\",products_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde367c9",
   "metadata": {},
   "source": [
    "### Getting Data --> Excel - xlsx | CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cd7c3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_CSV(file: str, products: List[Dict]):\n",
    "    \"\"\"\n",
    "    Convert a list of product dictionaries into a pandas DataFrame.\n",
    "    \n",
    "    Args:\n",
    "        products: List of product dictionaries containing scraped data\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: Structured dataframe containing all product information\n",
    "    \"\"\"\n",
    "    # Initialize dictionary to store data with more descriptive variable names\n",
    "    product_data = {\n",
    "        'brand': [],\n",
    "        'description': [],\n",
    "        'seller': [],\n",
    "        'price_amount': [],\n",
    "        'price_discount': [],\n",
    "        'rating': [],\n",
    "        'has_stars': [],  # More descriptive than just 'stars'\n",
    "        'total_sales': [],\n",
    "        'shipping_info': [],  # More descriptive\n",
    "        'image_url': [],  # Consistent naming\n",
    "        'product_url': []\n",
    "    }\n",
    "\n",
    "    # Extract data from each product\n",
    "    for product in products:\n",
    "        try:\n",
    "            product_data['brand'].append(product.get('brand', ''))\n",
    "            product_data['description'].append(product.get('description', ''))\n",
    "            product_data['seller'].append(product.get('seller', ''))\n",
    "            product_data['price_amount'].append(product.get('price', {}).get('amount', ''))\n",
    "            product_data['price_discount'].append(product.get('price', {}).get('discount', ''))\n",
    "            product_data['rating'].append(product.get('reviews', {}).get('rating', ''))\n",
    "            product_data['has_stars'].append(product.get('reviews', {}).get('stars', False))\n",
    "            product_data['total_sales'].append(product.get('reviews', {}).get('total_sales', ''))\n",
    "            product_data['shipping_info'].append(product.get('shipping', ''))\n",
    "            product_data['image_url'].append(product.get('img_url', ''))\n",
    "            product_data['product_url'].append(product.get('product_url', ''))\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing product: {e}\")\n",
    "            break\n",
    "\n",
    "    df = pd.DataFrame(product_data)\n",
    "    df.to_excel(f\"./products/dirty/{file}.xlsx\", engine='openpyxl',index=False, sheet_name='laptops')\n",
    "    df.to_csv(f\"./products/dirty/{file}.csv\", index=False, sep=\";\", encoding='utf-8-sig')\n",
    "\n",
    "get_CSV(\"laptops\",products_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrap",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
